# Knowledger ğŸ§ 

> An intelligent RAG (Retrieval-Augmented Generation) system that transforms your documents into a conversational knowledge base

## âœ¨ What Makes Knowledger Special

**Smart Document Understanding** - Upload PDFs and get instant semantic search capabilities across your entire document collection

**Lightning-Fast Responses** - Powered by Ollama's Gemma3 model with intelligent semantic caching for sub-second query responses

**Production-Ready Architecture** - Built with Redis vector storage, SentenceTransformers embeddings, and containerized deployment

## ğŸš€ Key Features

### ğŸ“„ Intelligent Document Processing
- **PDF Text Extraction** with smart chunking (500 words, 50-word overlap)
- **Semantic Embeddings** using `multi-qa-mpnet-base-dot-v1` for superior context understanding
- **Vector Storage** in Redis with RediSearch for millisecond retrieval

### ğŸ” Advanced Search & Retrieval
- **Semantic Search** - Find relevant content even with different wording
- **Context-Aware Responses** - Combines multiple document chunks for comprehensive answers
- **Smart Caching** - Semantic similarity-based caching reduces redundant LLM calls

### ğŸ¯ Modern Tech Stack
```
ğŸ³ Docker Compose    â†’  One-command deployment
ğŸ”— Redis Stack       â†’  Vector database with RediSearch
ğŸ¤– Ollama + Gemma3   â†’  Local LLM inference
ğŸ§  SentenceFormers   â†’  State-of-the-art embeddings
âš¡ Flask API          â†’  RESTful backend
ğŸ¨ Modern Web UI     â†’  Clean, responsive interface
```

## ğŸ— Architecture Overview

Knowledger implements a sophisticated RAG pipeline that intelligently processes documents and provides contextual answers:

**Document Ingestion Flow:**
PDF â†’ Text Extraction â†’ Intelligent Chunking â†’ Vector Embedding â†’ Redis Storage

**Query Processing Flow:**
User Question â†’ Embedding â†’ Vector Search â†’ Context Assembly â†’ LLM Generation â†’ Response

**Smart Features:**
- ğŸ¯ Similarity threshold filtering for relevant context
- ğŸ’¾ Semantic caching for frequently asked questions  
- ğŸ“Š Performance monitoring and health checks
- ğŸ”„ Real-time document indexing

## ğŸª Getting Started

Simply run with Docker Compose - everything is pre-configured:

```bash
docker-compose up
```

That's it! Your intelligent document assistant is ready at `http://localhost:5000`

## ğŸ’¡ Perfect For

- **Research Teams** - Query across multiple research papers instantly
- **Documentation** - Transform static docs into interactive knowledge bases  
- **Customer Support** - Build intelligent FAQ systems from your content
- **Content Analysis** - Discover insights across large document collections

---

## Works like this 
```mermaid
graph TD
    A["ğŸ‘¤ User"] --> B["ğŸŒ Web Interface"]
    A --> C["ğŸ“± API Client"]
    
    B --> D["âš¡ Flask API Server"]
    C --> D
    
    D --> E{{"ğŸ”€ Route Handler"}}
    
    E -->|"ğŸ“„ Upload PDF"| F["ğŸ“¤ Document Upload"]
    E -->|"â“ Ask Question"| G["ğŸ” Query Processing"]
    E -->|"ğŸ¥ Health Check"| H["ğŸ“Š System Status"]
    
    F --> I["ğŸ“– PDF Text Extraction"]
    I --> J["âœ‚ï¸ Text Chunking<br/>(500 words, 50 overlap)"]
    J --> K["ğŸ§  Generate Embeddings<br/>(SentenceTransformers)"]
    K --> L["ğŸ’¾ Store in Redis<br/>(Vector Database)"]
    L --> M["âœ… Upload Complete"]
    
    G --> N["ğŸ” Check Semantic Cache"]
    N -->|"âœ… Cache Hit"| O["âš¡ Return Cached Response"]
    N -->|"âŒ Cache Miss"| P["ğŸ§  Generate Query Embedding"]
    
    P --> Q["ğŸ” Vector Similarity Search<br/>(Redis RediSearch)"]
    Q --> R["ğŸ“‹ Retrieve Relevant Chunks<br/>(Similarity > 0.3)"]
    R --> S["ğŸ”— Assemble Context"]
    S --> T["ğŸ¤– Ollama + Gemma3<br/>(LLM Generation)"]
    T --> U["ğŸ’¾ Cache Response<br/>(Semantic Similarity)"]
    U --> V["ğŸ“¤ Return Answer"]
    
    L --> W[("ğŸ”— Redis Stack<br/>Vector Storage")]
    Q --> W
    U --> X[("ğŸ’° Semantic Cache<br/>Response Storage")]
    N --> X
    
    style A fill:#e1f5fe
    style D fill:#f3e5f5
    style W fill:#fff3e0
    style X fill:#e8f5e8
    style T fill:#fce4ec
```